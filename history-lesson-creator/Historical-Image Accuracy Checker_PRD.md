Title: Historical-Image Accuracy Checker. Overview: Automated chain using two VLMs on your RTX four-zero-nine-zero rig to score gen pics against era refs—throw out fails early, deep-dive the winners. Goals: Slash bad outputs, keep workflow snappy, spit out clean reports. User flow: Feed folder of images into Python script via Ollama API; first hit's Llama three-point-two Vision—prompt it, Please continue.to evaluate each one against a prompt you set, like "rate this image's historical accuracy from zero to one hundred for a nineteen twenties scene, justify." If it scores above eighty, send it straight to the second model—Qwen2.5VL—running the same thing but with a stricter prompt, maybe adding "be brutal, check tiny details." Both models spit out the score plus a short note, get saved in a JSON next to the image. At the end, script compiles all reports into a neat PDF summary—green for keepers, red for trash. Tech stack: Ollama for the models, Python three point eleven, use libraries like pillow for image handling, requests to talk to Ollama's port, maybe reportlab for PDFs. Inputs: a folder path, era keywords, threshold cutoffs. Outputs: same folder with JSONs and a final scorecard. Needs: your GPU driver up to date, at least thirty gig free space for the models.